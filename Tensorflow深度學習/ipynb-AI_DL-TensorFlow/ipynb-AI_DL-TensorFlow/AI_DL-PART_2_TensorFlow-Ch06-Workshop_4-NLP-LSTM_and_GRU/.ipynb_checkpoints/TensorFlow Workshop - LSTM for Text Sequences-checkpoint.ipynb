{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JY50zmDEa_q"
   },
   "source": [
    "# TensorFlow Workshop - LSTM for Text Sequences\n",
    "2019/08/14\n",
    "\n",
    "> [ Reference ] :\n",
    "1. Tom Hope, Yehezkel S. Resheff, and Itay Lieder, \"**`Learning TensorFlow : A Guide to Building Deep Learning Systems`**\", Chapter 5, O'Reilly, 2017.\n",
    "2. Victor Chou, \"**`An Introduction to Recurrent Neural Networks for Beginners`**\" Towards Data Science, 2019/07/25. https://towardsdatascience.com/an-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd\n",
    "3. Wikipedia, \"**`Long short-term memory`**\", 2019. https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Sequences\n",
    "+ Our simulated data consists of two classes of very short “sentences,” one composed of odd digits and the other of even digits (with numbers written in English). \n",
    "+ We generate sentences built of words representing even and odd numbers. \n",
    "\n",
    "\n",
    "+ **Our goal** is *to learn to classify each sentence as either odd or even in a supervised text-classification task*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\other\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# for the old-version usage of TensorFlow, such as tensorflow.examples.tutorials.mnist\n",
    "old_v = tf.logging.get_verbosity()          \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "times_steps = 6\n",
    "element_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Next, we create sentences. We sample random digits and map them to the corresponding\n",
    "“words” (e.g., 1 is mapped to “One,” 7 to “Seven,” etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n",
    "                     6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Text sequences typically have variable lengths, which is of course the case for all real natural language data (such as in the sentences appearing on this page).\n",
    "\n",
    "> + To make our simulated sentences have different lengths, we sample for each sentence a random length between 3 and 6 with **`np.random.choice(range(3, 7))`**—the lower bound is inclusive, and the upper bound is exclusive.\n",
    "\n",
    "\n",
    "> + Now, to put all our input sentences in one tensor (per batch of data instances), we need them to somehow be of the same size—so we pad sentences with a length shorter than 6 with zeros (or PAD symbols) to make all sentences equally sized (artificially). This pre-processing step is known as **zero-padding**. \n",
    "    \n",
    "The following code accomplishes all of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_word_map[0]=\"PAD\"\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),rand_seq_len)\n",
    "    \n",
    "    # Padding\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                  [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                  [0]*(6-rand_seq_len))\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] \n",
    "                                    for r in rand_even_ints]))\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] \n",
    "                                   for r in rand_odd_ints]))\n",
    "    \n",
    "data = even_sentences+odd_sentences\n",
    "# Same seq lengths for even, odd sentences\n",
    "seqlens*=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at our sentences, each padded to length 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eight Four Six Six Four PAD',\n",
       " 'Two Six Six Two Six Two',\n",
       " 'Six Eight Six Two PAD PAD',\n",
       " 'Four Two Two PAD PAD PAD',\n",
       " 'Two Eight Six Two PAD PAD',\n",
       " 'Eight Six Six PAD PAD PAD']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_sentences[0:6]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Five Nine Nine One One PAD',\n",
       " 'One Seven Seven One Three Seven',\n",
       " 'Three One Nine Three PAD PAD',\n",
       " 'Five Seven One PAD PAD PAD',\n",
       " 'Seven Five Three Nine PAD PAD',\n",
       " 'Nine Three Nine PAD PAD PAD']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_sentences[0:6]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + Notice that we add the **PAD** word (token) to our data and `digit_to_word_map` dictionary, and separately store even and odd sentences and their original lengths (before padding).\n",
    "\n",
    "Let’s take a look at the original sequence lengths for the sentences we printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 4, 3, 4, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlens[0:6]  # Same seq lengths for even, odd sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q : Why keep the original sentence lengths? \n",
    "> + By zero-padding, we solved one technical\n",
    "problem but created another: if we naively pass these padded sentences through our\n",
    "RNN model as they are, it will process useless **PAD** symbols. \n",
    "\n",
    "\n",
    "> + This would both harm model correctness by processing “*noise*” and increase computation time. We resolve this issue by first storing the original lengths in the seqlens array and then telling TensorFlow’s **`tf.nn.dynamic_rnn()`** where each sentence ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Our data is simulated—generated by us. In real applications, we would  start off by getting a collection of documents (e.g., one-sentence tweets) and then mapping each word to an integer ID.\n",
    "\n",
    "\n",
    "+ So, we now **map words to indices**—word `identifiers`—by simply creating a dictionary with words as keys and indices as values. \n",
    "+ We also create the **inverse map**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "            \n",
    "# Inverse map\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a supervised classification task—we need an array of labels in the `one-hot` format, train and test sets, a function to generate batches of instances, and placeholders, as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ First, we create the labels and split the data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Next, we create a function that generates batches of sentences. Each sentence in a\n",
    "batch is simply a list of integer IDs corresponding to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_batch(batch_size,data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()]\n",
    "    for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Finally, we create placeholders for data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "\n",
    "# seqlens for dynamic calculation\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   \n",
    "## 2. Supervised Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Our text data is now encoded as lists of word IDs—each sentence is a sequence of integers corresponding to words.\n",
    "+ **We could end up with millions of such word IDs, each encoded in `one-hot` (binary) categorical form, leading to great data sparsity and computational issues.**\n",
    "#### [ NOTE ]:\n",
    "> + A powerful approach to work around this issue is to use **word embeddings**, *which makes the high-dimensional one-hot vectors “embedded” into a continuous vector space with a much lower dimensionality*.\n",
    "> + In **Ref. 1's Chapter 6** we dive deeper into word embeddings, exploring a popular method to train them in an “*unsupervised*” manner known as **`word2vec`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here, our end goal is *to solve a text classification problem, and we will train word vectors in a supervised framework, tuning the embedded word vectors to solve the downstream classification task*.\n",
    "\n",
    "\n",
    "+ Previously, we gave each word an integer index, and sentences are then represented as sequences of these indices. \n",
    "\n",
    "\n",
    "#### `tf.nn.embedding_lookup() function`\n",
    "+ It is helpful to think of word embeddings as *basic hash tables* or *lookup tables*, mapping words to their dense vector values. These vectors are optimized as part of the training process.\n",
    "+ **Now, to obtain a word’s vector, we use the built-in `tf.nn.embedding_lookup()` function, which efficiently retrieves the vectors for each word in a given sequence of word indices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size,\n",
    "                               embedding_dimension],\n",
    "                               -1.0, 1.0),name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## 3. LSTM and Using Sequence Length\n",
    "\n",
    "+ A very popular recurrent network is the **`long short-term memory (LSTM) network`**. \n",
    ">    + It differs from vanilla RNN by having some special *memory mechanisms* that enable the recurrent cells to better store information for long periods of time, thus allowing them to capture long-term dependencies better than plain RNN.\n",
    "    + **These memory mechanisms simply consist of some more parameters added to each recurrent cell, enabling the RNN to overcome optimization issues and propagate information.** \n",
    "    + **These trainable parameters act as filters that select what information is worth “*remembering*” and passing on, and what is worth “*forgetting*.”**\n",
    "+ **They are trained in exactly the same way as any other parameter in a network, with gradient-descent algorithms and backpropagation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an LSTM cell with `tf.contrib.rnn.BasicLSTMCell()` and feed it to `tf.nn.dynamic_rnn()`.** \n",
    "\n",
    "+ We also give **`dynamic_rnn()`** the length of each sequence in a batch of examples, using the **`_seqlens`** placeholder we created earlier. \n",
    "    + TensorFlow uses this to stop all RNN steps beyond the last real sequence element. \n",
    "    + It also returns all output vectors over time (in the outputs tensor), which are all zero-padded beyond the true end of the sequence.\n",
    "\n",
    "\n",
    "+ So, for example, if the length of our original sequence is 5 and we zero-pad it to a sequence of length 15, the output for all time steps beyond 5 will be zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,\n",
    "                                              forget_bias=1.0)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)\n",
    "weights = {\n",
    "'linear_layer': tf.Variable(tf.truncated_normal([hidden_layer_size,\n",
    "                            num_classes], mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "'linear_layer':tf.Variable(tf.truncated_normal([num_classes],\n",
    "                           mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "# Extract the last relevant output and use in a linear layer\n",
    "final_output = tf.matmul(states[1],\n",
    "                         weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits = final_output,\n",
    "                                                  labels = _labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[ NOTE ] :  We take the last valid output vector — in this case conveniently available for us in the `states` tensor returned by `dynamic_rnn()` — and pass it through a linear layer (and the softmax function), using it as our final prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## 4. Training Embeddings and the LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1), tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 32.03125\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                         train_x,train_y, train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                       _seqlens:seqlen_batch})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                               _labels:y_batch,\n",
    "                                               _seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "        \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                        test_x,test_y,\n",
    "                                                        test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1), accuracy],\n",
    "                                         feed_dict={_inputs:x_test,\n",
    "                                                    _labels:y_test,\n",
    "                                                    _seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "    \n",
    "    output_example = sess.run([outputs],feed_dict={_inputs:x_test,\n",
    "                                                   _labels:y_test,\n",
    "                                                   _seqlens:seqlen_test})\n",
    "    states_example = sess.run([states[1]],feed_dict={_inputs:x_test,\n",
    "                                                     _labels:y_test,\n",
    "                                                     _seqlens:seqlen_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let’s take a look at one example of these outputs, for a sentence that was zero-padded (in your random batch of data you may see different output, of course—look for a sentence whose seqlen was lower than the maximal 6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlen_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This output has, as expected, six time steps, each a vector of size 32. \n",
    "\n",
    "Let’s take a glimpse at its values (printing only the first few dimensions to avoid clutter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45283183,  0.18316785, -0.41409463],\n",
       "       [-0.759822  ,  0.3627894 , -0.73591554],\n",
       "       [-0.86810327,  0.5210694 , -0.8492956 ],\n",
       "       [-0.8953353 ,  0.59556055, -0.88158625],\n",
       "       [-0.9370094 ,  0.6916338 , -0.86893153],\n",
       "       [ 0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example[0][1][:6,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that for this sentence, whose original length was 5, the last one time step have zero vectors due to padding.\n",
    "\n",
    "Finally, we look at the states vector returned by `dynamic_rnn()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9370094 ,  0.6916338 , -0.86893153], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_example[0][1][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **We can see that it conveniently stores for us the last relevant output vector — its values match the last relevant output vector before zero-padding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, you may be wondering how to access and manipulate the word vectors and explore the trained representation. \n",
    "> [ Hint ] : How to do so, including interactive embedding visualization, please refer to **Ref. 1's Chapter 6**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "## [ EXERCISE ] : Stacking multiple LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Earlier, we focused on a one-layer LSTM network for ease of exposition. \n",
    "+ Adding more layers is straightforward, using the **`MultiRNNCell()`** wrapper that combines multiple RNN cells into one multilayer cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, for example, we wanted to stack two LSTM layers in the preceding example. We\n",
    "can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_LSTM_layers = 2\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,\n",
    "                                             forget_bias=1.0)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers,\n",
    "                                       state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We first define an LSTM cell as before, and then feed it into the `tf.contrib.rnn.MultiRNNCell()` wrapper.\n",
    "\n",
    "+ Now our network has two layers of LSTM, causing some shape issues when trying to\n",
    "extract the final state vectors. To get the final state of the second layer, we simply\n",
    "adapt our indexing a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states[num_LSTM_layers-1][1],\n",
    "                         weights[\"linear_layer\"]) + biases[\"linear_layer\"]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Chapter5.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
