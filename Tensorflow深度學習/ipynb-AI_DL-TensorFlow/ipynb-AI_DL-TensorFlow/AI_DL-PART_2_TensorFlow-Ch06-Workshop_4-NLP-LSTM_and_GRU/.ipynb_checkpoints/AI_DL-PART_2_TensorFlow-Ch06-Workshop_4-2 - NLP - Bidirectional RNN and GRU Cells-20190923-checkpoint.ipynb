{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JY50zmDEa_q"
   },
   "source": [
    "# PART 2 TensorFlow\n",
    "# 6. Workshop 4-2 :  自然語言處理 (NLP) - Bidirectional RNN and GRU Cells\n",
    "2019/08/30\n",
    "\n",
    "> [ Reference ] :\n",
    "1. Tom Hope, Yehezkel S. Resheff, and Itay Lieder, \"**`Learning TensorFlow : A Guide to Building Deep Learning Systems`**\", Chapter 6, O'Reilly, 2017.\n",
    "      [ Code ] : https://github.com/giser-yugang/Learning_TensorFlow\n",
    "2. Victor Zhou, \"**`An Introduction to Recurrent Neural Networks for Beginners`**\" Towards Data Science, 2019/07/25. https://towardsdatascience.com/an-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd\n",
    "3. Andrej Karpathy, \"**`The Unreasonable Effectiveness of Recurrent Neural Networks`**\" Andrej Karpathy blog, 2015/05/21. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "4. Wikipedia, \"**`Long short-term memory`**\", 2019. https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "5. 陳誠, \"**人人都能看懂的GRU**\", https://zhuanlan.zhihu.com/p/32481747\n",
    "6. Wikipedia, \"**`Gated recurrent unit`**\" https://en.wikipedia.org/wiki/Gated_recurrent_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Gated recurrent units (GRUs)** are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho *et al*. \n",
    "+ The GRU is like a `long short-term memory (LSTM) with forget gate` but has *fewer parameters than LSTM*, as **it lacks an output gate**. \n",
    "+ **GRUs have been shown to exhibit even better performance on certain smaller datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "![title](./GRU-Fig_1-GRUs_model.png)\n",
    "\n",
    "**Figure 1 The GRU model.** (from Ref. 5)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "![title](./GRU-Fig_2-Fully_Gated_Unit.png)\n",
    "\n",
    "----------------------\n",
    "**Figure 2 GRU - Fully Gated Unit.** (from Ref. 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Embeddings, Advanced RNN \n",
    "\n",
    "> [ Reference ]\n",
    "1. Tom Hope, Yehezkel S. Resheff, and Itay Lieder, \"**`Learning TensorFlow : A Guide to Building Deep Learning Systems`**\", *\"`Bidirectional RNN and GRU Cells`\"* in Chapter 6, O'Reilly, 2017.\n",
    "2. Wikipedia, \"**`Gated recurrent unit`**\" https://en.wikipedia.org/wiki/Gated_recurrent_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pretrained embeddings : `glove.840B.300d.zip` (2.03 GB)\n",
    "+ https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# for the old-version usage of TensorFlow, such as tensorflow.examples.tutorials.mnist\n",
    "old_v = tf.logging.get_verbosity()          \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pretrained embeddings\n",
    "def get_glove(path_to_glove,word2index_map):\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0\n",
    "    \n",
    "    with open(os.path.join(path_to_glove, 'glove.840B.300d.txt'), encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            vals = line.split()\n",
    "            word = str(vals[0])\n",
    "            if word in word2index_map:\n",
    "                print(word)\n",
    "                count_all_words+=1\n",
    "                coefs = np.array(vals[1:],dtype='float32')\n",
    "                coefs/=np.linalg.norm(coefs)\n",
    "                embedding_weights[word]=coefs\n",
    "            if count_all_words==vocabulary_size-1:\n",
    "                break\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_batch(batch_size,data_x,data_y,data_seqlens):\n",
    "    instance_indeces = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indeces)\n",
    "    batch = instance_indeces[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN and GRU Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove = './glove.840B.300d'\n",
    "pre_trained = True\n",
    "glove_size = 300\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_class =2\n",
    "hidden_layer_size = 32\n",
    "time_steps = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_word_map = {1: 'One', 2: 'Two', 3: 'Three', 4: 'Four', 5: 'Five'\n",
    "        , 6: 'Six', 7: 'Seven', 8: 'Eight', 9: 'Nine'}\n",
    "digit_to_word_map[0] = 'PAD_token'\n",
    "\n",
    "even_sentences =[]\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),rand_seq_len)\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,[0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,[0]*(6-rand_seq_len))\n",
    "    even_sentences.append(' '.join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    odd_sentences.append(' '.join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "\n",
    "data = even_sentences+odd_sentences\n",
    "seqlens*=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "Two\n",
      "Three\n",
      "Four\n",
      "Five\n",
      "Six\n",
      "Seven\n",
      "Nine\n",
      "Eight\n"
     ]
    }
   ],
   "source": [
    "labels = [1]*10000+[0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label]=1\n",
    "    labels[i] = one_hot_encoding\n",
    "\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "\n",
    "index2word_map = {index:word for word,index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "word2bedding_dict = get_glove(path_to_glove,word2index_map)\n",
    "embedding_matrix = np.zeros((vocabulary_size,glove_size))\n",
    "for word,index in word2index_map.items():\n",
    "    if not word =='PAD_token':\n",
    "        word_embedding = word2bedding_dict[word]\n",
    "        embedding_matrix[index,:]=word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train,test\n",
    "data_indeces = list(range(len(data)))\n",
    "np.random.shuffle(data_indeces)\n",
    "data = np.array(data)[data_indeces]\n",
    "labels = np.array(labels)[data_indeces]\n",
    "seqlens = np.array(seqlens)[data_indeces]\n",
    "\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------\n",
    "##  Building the computation Graph\n",
    "## ----------------------------------\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32,shape=[batch_size,time_steps])\n",
    "embedding_placeholder = tf.placeholder(tf.float32,[vocabulary_size,glove_size])\n",
    "_labels = tf.placeholder(tf.float32,shape=[batch_size,num_class])\n",
    "_seqlens = tf.placeholder(tf.int32,shape=[batch_size])\n",
    "\n",
    "# Note that we created an embedding_placeholder, to which we feed the word vectors.\n",
    "if pre_trained:\n",
    "    embeddings = tf.Variable(tf.constant(0.0,shape = [vocabulary_size,glove_size])\n",
    "                                 ,trainable=True)\n",
    "    # If using pretrained embeddings, assign them to the embeddings variable\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,_inputs)\n",
    "else:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_dimension],-1.0,1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings,_inputs)\n",
    "\n",
    "## -----------------------------------\n",
    "##   Bidirectional RNN and GRU Cells\n",
    "## -----------------------------------\n",
    "    \n",
    "with tf.name_scope('biGRU'):\n",
    "    with tf.variable_scope('forward'):\n",
    "        gru_fw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell)\n",
    "    with tf.variable_scope('backward'):\n",
    "        gru_bw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell)\n",
    "\n",
    "    outputs,states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_bw=gru_bw_cell,cell_fw=gru_fw_cell,inputs=embed,\n",
    "        sequence_length=_seqlens,dtype=tf.float32,scope='BiGRU')\n",
    "states = tf.concat(values=states,axis=1)\n",
    "\n",
    "weights = {'linear_layer':tf.Variable(tf.truncated_normal([2*hidden_layer_size,num_class],\n",
    "                                                              mean=0,stddev=.01))}\n",
    "biases = {'linear_layer':tf.Variable(tf.truncated_normal([num_class],\n",
    "                                                             mean=0,stddev=.01))}\n",
    "final_output = tf.matmul(states,weights['linear_layer'])+biases['linear_layer']\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output,labels=_labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "\n",
    "train_step = tf.train.RMSPropOptimizer(0.001,0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1),tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction,tf.float32)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ast 0: 88.28125\n",
      "Accuracy ast 100: 79.68750\n",
      "Accuracy ast 200: 100.00000\n",
      "Accuracy ast 300: 100.00000\n",
      "Accuracy ast 400: 100.00000\n",
      "Accuracy ast 500: 100.00000\n",
      "Accuracy ast 600: 100.00000\n",
      "Accuracy ast 700: 100.00000\n",
      "Accuracy ast 800: 100.00000\n",
      "Accuracy ast 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "## ----------------------------------\n",
    "##   Launching the Graph\n",
    "## ----------------------------------\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init,feed_dict={embedding_placeholder:embedding_matrix})\n",
    "\n",
    "    for step in range(1000):\n",
    "        x_batch,y_batch,seqlens_batch = get_sentence_batch(\n",
    "                batch_size,train_x,train_y,train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch,_labels:y_batch,_seqlens:seqlens_batch})\n",
    "        if step%100==0:\n",
    "            acc = sess.run(accuracy,\n",
    "                               feed_dict={_inputs:x_batch,_labels:y_batch,_seqlens:seqlens_batch})\n",
    "            print('Accuracy ast %d: %.5f'%(step,acc))\n",
    "\n",
    "    for test_batch in range(5):\n",
    "        x_test,y_test,seqlen_test = get_sentence_batch(batch_size,test_x,test_y,test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1),accuracy],\n",
    "                                            feed_dict={_inputs:x_test,_labels:y_test,_seqlens:seqlen_test})\n",
    "        print('Test batch accuracy %d: %.5f'%(test_batch,batch_acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Chapter5.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
